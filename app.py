# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OfJmE9YsTAbzMi_Vn6AMZ-5MHAJTAZPp
"""

!pip install gradio huggingface_hub pandas scikit-learn --quiet

import os

# Set your Hugging Face API Key securely
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "sir i removed api key for security "

from huggingface_hub import InferenceClient

# Use a lightweight model for faster Colab execution
model_name = "google/flan-t5-base"  # You can change to Mixtral if Colab Pro
client = InferenceClient(model=model_name, token=os.environ["HUGGINGFACEHUB_API_TOKEN"])

# Chat with AI doctor
def chat_with_ai(message):
    prompt = f"You are a helpful medical assistant. Patient says: {message}\nRespond:"
    response = client.text_generation(prompt, max_new_tokens=150)
    return response.strip()

# Disease prediction (basic rule-based)
def predict_disease(symptoms):
    symptoms = symptoms.lower()
    if "fever" in symptoms and "cough" in symptoms:
        return "Likely: Flu"
    elif "headache" in symptoms and "nausea" in symptoms:
        return "Likely: Migraine"
    elif "sore throat" in symptoms and "fatigue" in symptoms:
        return "Likely: COVID-19"
    else:
        return "No clear match. Please consult a doctor."

# Treatment plan generation
def generate_treatment(disease):
    prompt = f"Suggest a treatment plan for a patient diagnosed with {disease}."
    response = client.text_generation(prompt, max_new_tokens=150)
    return response.strip()

import gradio as gr

with gr.Blocks(title="HealthAI - Gradio") as demo:
    gr.Markdown("# üß† HealthAI - Medical Assistant (Gradio + HF)")

    with gr.Tab("ü§ñ Chat with AI Doctor"):
        user_msg = gr.Textbox(label="Your Message")
        ai_reply = gr.Textbox(label="AI Response")
        user_msg.submit(chat_with_ai, inputs=user_msg, outputs=ai_reply)

    with gr.Tab("üîç Disease Predictor"):
        symptom_input = gr.Textbox(label="Enter symptoms (e.g., fever, cough)")
        predicted = gr.Textbox(label="Prediction")
        symptom_input.submit(predict_disease, inputs=symptom_input, outputs=predicted)

    with gr.Tab("üíä Treatment Generator"):
        disease_name = gr.Textbox(label="Enter disease name")
        treatment_plan = gr.Textbox(label="Suggested Treatment")
        disease_name.submit(generate_treatment, inputs=disease_name, outputs=treatment_plan)

from huggingface_hub import InferenceClient
import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "removed api key "
client = InferenceClient(model="google/flan-t5-base", token=os.environ["HUGGINGFACEHUB_API_TOKEN"])

def chat_with_ai(message):
    prompt = f"Answer medically: {message}"
    response = client.text_generation(prompt, max_new_tokens=100)
    return response.strip()

demo.launch(share=True)  # share=True gives you a public link

